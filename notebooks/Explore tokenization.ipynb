{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbed21d-7d97-4f5c-8b67-5f9615a6fd19",
   "metadata": {},
   "source": [
    "**Explore tokenization**\n",
    "\n",
    "In the notebook 'Iterative Prompt using Chat-GPT' the model 'text-davinci-003' was selected for the sentiment clasification of the spanish sentences.\n",
    "\n",
    "The model used defines that requests can use up to 4000 tokens shared between prompt and completion.\n",
    "\n",
    "Let's explore how tokens can be counted programmatically count tokens.\n",
    "\n",
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54444e81-cfa5-4bb9-9b05-44fae8e89050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa920e94-d477-4dfe-8de2-dce9d7b2b55e",
   "metadata": {},
   "source": [
    "**Read the CSV file with all 'versos al paso'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4551f7a-03aa-4f57-9e8b-f33435d27254",
   "metadata": {},
   "source": [
    "Load CSV file, show columns and other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535c0488-18b1-4889-aa35-955dfa0f2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "versos_al_paso_file_path = './input/versosalpaso.csv'\n",
    "versos_al_paso = pd.read_csv(versos_al_paso_file_path, sep=\"|\", encoding='utf-8')\n",
    "spanish_sentences = versos_al_paso.verso.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb067f-8755-4286-a122-65d215d483bc",
   "metadata": {},
   "source": [
    "**OpenAI tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e23f41-b645-471f-8c44-ab2bc8e44ac0",
   "metadata": {},
   "source": [
    "The following prompt was defined in the 'Iterative Prompt using Chat-GPT' notebook,\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following Spanish sentences,\n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Classify sentences according to their sentiment.\n",
    "\n",
    "The sentiment will be as a single word, \\\n",
    "either \"positive\"or \"neutral\" or \"negative\".\n",
    "\n",
    "Give your answer as JSON, where the key is the sentiment and the value is the list.\n",
    "\n",
    "Review text: '''{spanish_sentences}'''\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "as well as the following request\n",
    "\n",
    "```python\n",
    "def get_completion(prompt, model=\"text-davinci-003\"):\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    return response.choices[0].text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58777282-c9a7-486f-9cdb-6f0f3a2b1878",
   "metadata": {},
   "source": [
    "The model used defines that requests can use up to 4000 tokens shared between prompt and completion. It is necessary that the question does not exceed a maximum of 50% of the available tokens for which [tokens must be counted](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc262f9a-a2e0-48b9-bf65-3e6e2b75f1d0",
   "metadata": {},
   "source": [
    "Using OpenAI's interactive [Tokenizer tool](https://platform.openai.com/tokenizer), the following prompt with an empty list to review has **85 tokens** and 343 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea680dd-64f0-45dd-ba16-bd912c2122d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    What is the sentiment of the following Spanish sentences,\n",
      "    which is delimited with triple backticks?\n",
      "\n",
      "    Classify sentences according to their sentiment.\n",
      "\n",
      "    The sentiment will be as a single word, either \"positive\"or \"neutral\" or \"negative\".\n",
      "\n",
      "    Give your answer as JSON, where the key is the sentiment and the value is the list.\n",
      "\n",
      "    Review text: '''[]'''\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def get_prompt(spanish_sentences: list = []) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    What is the sentiment of the following Spanish sentences,\n",
    "    which is delimited with triple backticks?\n",
    "\n",
    "    Classify sentences according to their sentiment.\n",
    "\n",
    "    The sentiment will be as a single word, either \"positive\"or \"neutral\" or \"negative\".\n",
    "\n",
    "    Give your answer as JSON, where the key is the sentiment and the value is the list.\n",
    "\n",
    "    Review text: '''{spanish_sentences}'''\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "    \n",
    "print(get_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd648312-0c39-41c4-afcd-3de0998bb20d",
   "metadata": {},
   "source": [
    "Let's take a few sentences to test tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b7ea88-1980-4c08-8760-655c4f1326a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quizá el secreto de la vida tan solo consista En tener un lugar al que regresar', 'Cuando andamos cabeza abajo entonces es cuando empezamos a vivir de nuevo.', 'El bosque es el primigenio nudo iniciático, el hogar de los hombres perdidos.', 'Mi garganta es una gruta deslizante de acentos. La forma musical para sanar.', 'Hoy somos más viejos que nunca, piensas; pero nunca serás ya tan joven como hoy.', 'Dudarás: ¿Qué razón en tocar sus lunares? Pensará: Si lloro, diré \\\\', 'Quien olvida abandona, mata, entierra la memoria. Por ello, nunca olvido', 'No puede reparase con las manos una tela de araña. No hay dedos tan exquisitos', 'La política tiene colgado el cartel de rebajas', 'Queda de ti un vestido de aguas a la deriva, queda de ti tu nombre que no digo', 'Yo seré mimoso, pero tú serás mi musa', 'Soy origen y descendencia, soy luz, sombra y herencia, soy mujer soy resiliencia', 'En memoria de las auténticas cebras, las que en el Mioceno galopaban por Madrid', 'Nacer fue algo oblicuo. No se incendia el bosque sino el interior de cada árbol.', 'Peladitos ven TV y quieren ser así, están aprendiendo a matar antes que a vivir', 'Donde la distancia entre la tierra y nuestras manos era la medida de todas las cosas', 'La voz que tripula este barco navega sin olas.', 'Por ti, que eres cuerpo sostenido en un alma bemol sale Sol, brilla La, sale Sol', 'Amor es llegar en el momento oportuno y ser del tamaño del hueco vacío del otro.', 'Sola, inmersa en el ruido de la ciudad, oigo el silencio de la vida arrostrándose', 'Somos préstamos de calor', 'Te espero en la orilla del sueño de siempre', 'Nuestros castillos de arena están a salvo: se ha secado el mar de dudas', 'Y tú, ¿a cuál de todas tus cicatrices le escribes?', 'Que el teu camí sigui llarg, i ben aviat entenguis que la ciutat és casa.']\n"
     ]
    }
   ],
   "source": [
    "split_no = 25\n",
    "some_spanish_sentences = spanish_sentences[0:split_no]\n",
    "print(some_spanish_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e458752-83c1-4405-94f8-b7d91cb70d2d",
   "metadata": {},
   "source": [
    ", **688 tokens** and 1815 characters for the above set of sentences. The full request would have **772 tokens** and 2156 characters.\n",
    "\n",
    "Let's use OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library to count tokens as shown in [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47d6b9-e974-450f-9c1b-e62de56d3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ce6c04-b8b7-4af7-b8af-6247c992f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str = \"text-davinci-003\") -> int:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    assert enc.decode(enc.encode(string)) == string\n",
    "    num_tokens = len(enc.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f347b9-8645-4bda-a55a-68a4d049e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    empty request 90 tokens 373 characters\n",
      "    sentences: 688 tokens 1815 characters\n",
      "    request: 777 tokens 2186 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_request = get_prompt()\n",
    "sentences_str = f\"{some_spanish_sentences}\"\n",
    "full_request = get_prompt(some_spanish_sentences)\n",
    "print(f\"\"\"\n",
    "    empty request {num_tokens_from_string(empty_request)} tokens {len(empty_request)} characters\n",
    "    sentences: {num_tokens_from_string(sentences_str)} tokens {len(sentences_str)} characters\n",
    "    request: {num_tokens_from_string(full_request)} tokens {len(full_request)} characters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ee5c0-7915-48a6-b65d-057c3cd272cf",
   "metadata": {},
   "source": [
    "The expected completion will be a JSON object as a string as following\n",
    "\n",
    "```text\n",
    "{\"positive\": [], \"neutral\": [], \"negative\": []}\n",
    "```\n",
    "\n",
    "Let's count its tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc66231-c94c-4629-b7b7-764aa1fd0722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    15 tokens 47 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_completion = '{\"positive\": [], \"neutral\": [], \"negative\": []}'\n",
    "print(f\"\"\"\n",
    "    {num_tokens_from_string(expected_completion)} tokens {len(expected_completion)} characters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15332ffc-747b-4ea2-a03c-676506480d2f",
   "metadata": {},
   "source": [
    "One empty request and one empty completion are 105 tokens.\n",
    "\n",
    "Let's do some maths\n",
    "\n",
    "tokens available:\n",
    "4,000 - 105 = 3,895\n",
    "\n",
    "maximum number of tokens to be reviewed:\n",
    "3,895 / 2 = 1,947.5\n",
    "\n",
    "percent:\n",
    "1,947.5 / 4,000 = 48,69%\n",
    "\n",
    "Based on these results, this method will allow us to group the sentences in a way that does not exceed 48% of the tokens allowed in the request. A **more conservative 45% or 1,800 tokens** limit may be a better idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc763a68-b076-4f30-99f5-6a11f525ec48",
   "metadata": {},
   "source": [
    "**Split verses to request sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2463a4dc-5050-4709-b44c-7d75eaa2039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy\n",
    "\n",
    "def split_list(sentences: list = [], tokens_limit: int = 1800) -> list:\n",
    "    no_of_sentences = len(sentences)\n",
    "\n",
    "    from_pos = 0\n",
    "    splited_list = []\n",
    "    while True:\n",
    "        sentence = sentences[from_pos:from_pos+1][0]\n",
    "        \n",
    "        a_token_intent = num_tokens_from_string(f'{sentence}')\n",
    "        sentences_for_request = (tokens_limit // a_token_intent) + 1\n",
    "    \n",
    "        to_pos= from_pos + sentences_for_request\n",
    "    \n",
    "        while True:\n",
    "            sentences_to_request = sentences[from_pos:to_pos]\n",
    "            no_of_tokens = num_tokens_from_string(f'{sentences_to_request}')\n",
    "    \n",
    "            if tokens_limit < no_of_tokens:\n",
    "                while tokens_limit < no_of_tokens:\n",
    "                    no_of_tokens -= num_tokens_from_string(sentences_to_request[-1])\n",
    "                    sentences_to_request.pop()\n",
    "                break\n",
    "            else:\n",
    "                to_pos+= 1\n",
    "                if to_pos >= no_of_sentences:\n",
    "                    break\n",
    "    \n",
    "        splited_list.append(sentences_to_request)\n",
    "        \n",
    "        from_pos += len(sentences_to_request)\n",
    "        if from_pos >= no_of_sentences:\n",
    "            break\n",
    "\n",
    "    return splited_list\n",
    "\n",
    "\n",
    "splitted_list = split_list(spanish_sentences)\n",
    "requested_sentences = list(numpy.concatenate(splitted_list).flat)\n",
    "\n",
    "tc = unittest.TestCase()\n",
    "tc.assertListEqual(requested_sentences, spanish_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
