{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbed21d-7d97-4f5c-8b67-5f9615a6fd19",
   "metadata": {},
   "source": [
    "**What is the sentiment for the verses of the crosswalks in Madrid using OpenAI?**\n",
    "\n",
    "Classify verses by the associated sentiment using OpenAI.\n",
    "\n",
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae8434-8490-4a3e-a06a-b663fa6d5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7405da8-7281-4dc9-b0bb-f4243646c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54444e81-cfa5-4bb9-9b05-44fae8e89050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48580f0-717f-44f9-9459-990e340b9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa920e94-d477-4dfe-8de2-dce9d7b2b55e",
   "metadata": {},
   "source": [
    "**Read the CSV file with all 'versos al paso'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4551f7a-03aa-4f57-9e8b-f33435d27254",
   "metadata": {},
   "source": [
    "Load CSV file, show columns and other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535c0488-18b1-4889-aa35-955dfa0f2cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'latitud', 'longitud', 'autor', 'barrio', 'verso', 'direccion'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "versos_al_paso_file_path = './input/versosalpaso.csv'\n",
    "versos_al_paso = pd.read_csv(versos_al_paso_file_path, sep=\"|\", encoding='utf-8')\n",
    "versos_al_paso.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc17555-a1ee-4e18-8c04-45267188b2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['581256 bytes',\n",
       " '1100 rows',\n",
       " 'QuizÃ¡ el secreto de la vida tan solo consista En tener un lugar al que regresar']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "memory_in_use = np.sum(versos_al_paso.memory_usage(True, True).values)\n",
    "\n",
    "no_of_rows = len(versos_al_paso.index)\n",
    "a_verse = versos_al_paso.loc[0: 0].verso.values[0]\n",
    "\n",
    "[f'{memory_in_use} bytes', f'{no_of_rows} rows', a_verse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb492e6-9b0d-4f61-996a-662c39115147",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_sentences = versos_al_paso.verso.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb067f-8755-4286-a122-65d215d483bc",
   "metadata": {},
   "source": [
    "**Classifying with OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231773f8-b445-4449-8c74-5023042cbdf7",
   "metadata": {},
   "source": [
    "Read the secrect key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ad78fe-47f6-465a-b305-d4bac76d034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166178f-ae30-48df-a37f-661d7cf1f40a",
   "metadata": {},
   "source": [
    "The [Completions API](https://platform.openai.com/docs/guides/gpt/completions-api) and the '**text-davinci-003**' model are used as can see in the Jupyter Notebook 'Iterative Prompt using Chat-GPT'.\n",
    "\n",
    "Let's copy the required elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66764da3-2033-45f5-a895-ab344aa95e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, model: str = \"text-davinci-003\") -> str:\n",
    "    response = openai.Completion.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827683be-4ccf-40b3-adb4-446ff12138d1",
   "metadata": {},
   "source": [
    "The model used has a [rate limit](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api) as free trial user.\n",
    "\n",
    "In this case rate limits are:\n",
    "\n",
    "| Model            | RPM |     TPM |\n",
    "| ---------------- | ---:| -------:|\n",
    "| TEXT             |     |         |\n",
    "| text-davinci-003 |  60 | 150.000 |\n",
    "\n",
    "Under these limits there should be no problem in making requests for the 1100 phrases that make up the 'versos al paso'.\n",
    "\n",
    "Let's copy the code for batch processing according to the token limit in each request as explored in Jupyter Notebook 'Explore tokenization' and adding the prompt to use from the Jupyter Notebook above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b7a0a7-f45b-493e-a23b-079b4af3b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str = \"text-davinci-003\") -> int:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    assert enc.decode(enc.encode(string)) == string\n",
    "    num_tokens = len(enc.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10cacd19-1d41-429c-b7b2-17f7fa69f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(sentences: list, tokens_limit: int = 1800) -> list:\n",
    "    no_of_sentences = len(sentences)\n",
    "\n",
    "    from_pos = 0\n",
    "    splited_list = []\n",
    "    while True:\n",
    "        sentence = sentences[from_pos:from_pos+1][0]\n",
    "        \n",
    "        a_token_intent = num_tokens_from_string(f'{sentence}')\n",
    "        sentences_for_request = (tokens_limit // a_token_intent) + 1\n",
    "    \n",
    "        to_pos= from_pos + sentences_for_request\n",
    "    \n",
    "        while True:\n",
    "            sentences_to_request = sentences[from_pos:to_pos]\n",
    "            no_of_tokens = num_tokens_from_string(f'{sentences_to_request}')\n",
    "    \n",
    "            if tokens_limit < no_of_tokens:\n",
    "                while tokens_limit < no_of_tokens:\n",
    "                    no_of_tokens -= num_tokens_from_string(sentences_to_request[-1])\n",
    "                    sentences_to_request.pop()\n",
    "                break\n",
    "            else:\n",
    "                to_pos+= 1\n",
    "                if to_pos >= no_of_sentences:\n",
    "                    break\n",
    "    \n",
    "        splited_list.append(sentences_to_request)\n",
    "        \n",
    "        from_pos += len(sentences_to_request)\n",
    "        if from_pos >= no_of_sentences:\n",
    "            break\n",
    "\n",
    "    return splited_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0d25a7-07cf-472e-8dc9-3503b841698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def fix_xinvalid(m):\n",
    "    return chr(int(m.group(1), 16))\n",
    "\n",
    "def fix_escape(str):\n",
    "    xinvalid = re.compile(r'\\\\x([0-9a-fA-F]{2})')\n",
    "    return xinvalid.sub(fix_xinvalid, str)\n",
    "\n",
    "def get_sentiments(splitted_list: list) -> json:\n",
    "    sentiments_json = {}\n",
    "    for sentences in splitted_list:\n",
    "        prompt = f\"\"\"\n",
    "        What is the sentiment of the following Spanish sentences,\n",
    "        which is delimited with triple backticks?\n",
    "        \n",
    "        Classify sentences according to their sentiment.\n",
    "        \n",
    "        The sentiment will be as a single word, \\\n",
    "        either \"positive\" or \"neutral\" or \"negative\".\n",
    "        \n",
    "        Give your answer as JSON, where the key is the sentiment and the value is the list.\n",
    "        \n",
    "        Review text: '''{sentences}'''\n",
    "        \"\"\"\n",
    "        response = get_completion(prompt)\n",
    "    \n",
    "        try:\n",
    "            json_sentiments = json.loads(fix_escape(response))\n",
    "            for key, value in json_sentiments.items():\n",
    "                if key in sentiments_json:\n",
    "                    sentiments_json[key] += value\n",
    "                else:\n",
    "                    sentiments_json[key] = value\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if 'error' in sentiments_json:\n",
    "                sentiments_json['error'] += sentences\n",
    "            else:\n",
    "                sentiments_json['error'] = sentences\n",
    "\n",
    "    return sentiments_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232175a1-ef1d-4045-ac8e-d01a346fc1ed",
   "metadata": {},
   "source": [
    "**Save the result as a CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0805465a-5061-46e1-b6e3-95733d7eaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "versos_al_paso_sentiment = versos_al_paso.copy()\n",
    "versos_al_paso_sentiment['sentiment'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71131a8f-6ac0-4d04-8cac-b25a9d23151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sentiment(sentiments_json: json, versos_pd: pd.core.frame.DataFrame):\n",
    "    for key, value in sentiments_json.items():\n",
    "        if 'error' != key:\n",
    "            for sentence in value:\n",
    "                idx = versos_pd.verso.eq(sentence)\n",
    "                versos_pd.loc[idx, 'sentiment'] = key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654c97b-fdcb-44d2-9641-f5b04719b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_list = split_list(spanish_sentences)\n",
    "sentiments_json = get_sentiments(splitted_list)\n",
    "update_sentiment(sentiments_json, versos_al_paso_sentiment)\n",
    "\n",
    "versos_al_paso_sentiment_file_path = './output/versosalpaso_sentiment_text-davinci-003.csv'\n",
    "versos_al_paso_sentiment.to_csv(versos_al_paso_sentiment_file_path, sep=';', encoding='utf-8')\n",
    "\n",
    "if 'error' in sentiments_json:\n",
    "    assert [] == sentiments_json['error']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
